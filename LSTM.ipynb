{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEAN</th>\n",
       "      <th>MAX</th>\n",
       "      <th>MIN</th>\n",
       "      <th>RANGE</th>\n",
       "      <th>KURT</th>\n",
       "      <th>SKEW</th>\n",
       "      <th>MEAN_1ST_GRAD</th>\n",
       "      <th>STD_1ST_GRAD</th>\n",
       "      <th>MEAN_2ND_GRAD</th>\n",
       "      <th>STD_2ND_GRAD</th>\n",
       "      <th>...</th>\n",
       "      <th>MIN_ONSET_LOG</th>\n",
       "      <th>MIN_ONSET_SQRT</th>\n",
       "      <th>MAX_ONSET_LOG</th>\n",
       "      <th>MAX_ONSET_SQRT</th>\n",
       "      <th>STD_ONSET_YEO_JON</th>\n",
       "      <th>MEAN_ONSET_LOG</th>\n",
       "      <th>MEAN_ONSET_SQRT</th>\n",
       "      <th>NasaTLX</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.543036</td>\n",
       "      <td>0.965257</td>\n",
       "      <td>1.850634e-05</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721074</td>\n",
       "      <td>1.027930</td>\n",
       "      <td>3.154608</td>\n",
       "      <td>4.737494</td>\n",
       "      <td>1.456944</td>\n",
       "      <td>1.625915</td>\n",
       "      <td>2.020660</td>\n",
       "      <td>27.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.950346</td>\n",
       "      <td>1.039783</td>\n",
       "      <td>-5.712810e-07</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721074</td>\n",
       "      <td>1.027930</td>\n",
       "      <td>3.084339</td>\n",
       "      <td>4.566512</td>\n",
       "      <td>1.381432</td>\n",
       "      <td>1.583538</td>\n",
       "      <td>1.967781</td>\n",
       "      <td>60.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003698</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>4.888562</td>\n",
       "      <td>2.204727</td>\n",
       "      <td>9.236140e-04</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>-0.002940</td>\n",
       "      <td>0.004265</td>\n",
       "      <td>...</td>\n",
       "      <td>2.529238</td>\n",
       "      <td>3.397638</td>\n",
       "      <td>4.231222</td>\n",
       "      <td>8.234153</td>\n",
       "      <td>2.852115</td>\n",
       "      <td>3.740398</td>\n",
       "      <td>6.412078</td>\n",
       "      <td>62.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>6.399756</td>\n",
       "      <td>2.479169</td>\n",
       "      <td>-1.615331e-03</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>...</td>\n",
       "      <td>2.529238</td>\n",
       "      <td>3.397638</td>\n",
       "      <td>4.203095</td>\n",
       "      <td>8.117454</td>\n",
       "      <td>2.521958</td>\n",
       "      <td>3.471639</td>\n",
       "      <td>5.584752</td>\n",
       "      <td>62.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.117454</td>\n",
       "      <td>1.000727</td>\n",
       "      <td>2.749421e-04</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.188968</td>\n",
       "      <td>1.511189</td>\n",
       "      <td>3.259410</td>\n",
       "      <td>5.003417</td>\n",
       "      <td>2.003418</td>\n",
       "      <td>2.540301</td>\n",
       "      <td>3.418112</td>\n",
       "      <td>73.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MEAN       MAX       MIN     RANGE      KURT      SKEW  MEAN_1ST_GRAD  \\\n",
       "0  0.000735  0.001764  0.000177  0.001587  0.543036  0.965257   1.850634e-05   \n",
       "1  0.000691  0.001764  0.000198  0.001566  0.950346  1.039783  -5.712810e-07   \n",
       "2  0.003698  0.010928  0.001505  0.009423  4.888562  2.204727   9.236140e-04   \n",
       "3  0.003681  0.010928  0.001689  0.009239  6.399756  2.479169  -1.615331e-03   \n",
       "4  0.002909  0.007620  0.000802  0.006818  0.117454  1.000727   2.749421e-04   \n",
       "\n",
       "   STD_1ST_GRAD  MEAN_2ND_GRAD  STD_2ND_GRAD  ...  MIN_ONSET_LOG  \\\n",
       "0      0.000306       0.000023      0.000262  ...       0.721074   \n",
       "1      0.000275       0.000003      0.000242  ...       0.721074   \n",
       "2      0.004557      -0.002940      0.004265  ...       2.529238   \n",
       "3      0.003109       0.001296      0.002037  ...       2.529238   \n",
       "4      0.001386       0.000273      0.001333  ...       1.188968   \n",
       "\n",
       "   MIN_ONSET_SQRT  MAX_ONSET_LOG  MAX_ONSET_SQRT  STD_ONSET_YEO_JON  \\\n",
       "0        1.027930       3.154608        4.737494           1.456944   \n",
       "1        1.027930       3.084339        4.566512           1.381432   \n",
       "2        3.397638       4.231222        8.234153           2.852115   \n",
       "3        3.397638       4.203095        8.117454           2.521958   \n",
       "4        1.511189       3.259410        5.003417           2.003418   \n",
       "\n",
       "   MEAN_ONSET_LOG  MEAN_ONSET_SQRT  NasaTLX  subject_id  condition  \n",
       "0        1.625915         2.020660     27.6         2.0          2  \n",
       "1        1.583538         1.967781     60.7         1.0          1  \n",
       "2        3.740398         6.412078     62.6         5.0          2  \n",
       "3        3.471639         5.584752     62.6         5.0          2  \n",
       "4        2.540301         3.418112     73.0         5.0          0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='condition', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUM0lEQVR4nO3df7BfdX3n8efLRGitrYC5UpqEvZk2xYm2FvYWsXQdlS0GtmuooxZGJcXsZH8Aa7cdXXR3ll27zNh1t6yoZSaVSGAolOIPsl12aQoqrcqPgIgkaLmDldwUyMUg1TriBt/7x/eT+iXcm3MT8v1+c7nPx8x37jnv8znnvDMX8sr58T0nVYUkSfvzglE3IEk6/BkWkqROhoUkqZNhIUnqZFhIkjotHnUDg7BkyZIaHx8fdRuSNK/cfffdj1fV2EzLnpdhMT4+ztatW0fdhiTNK0m+OdsyT0NJkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOj0vv8F9oP7xe64adQvPe3d/6NxRtyDpOfDIQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR18tZZSSNz6kdOHXULz3tfuPALh2Q7HllIkjoZFpKkTgMLiyQbk+xKcv8+9QuTfC3JtiT/ra/+viSTSb6e5I199dWtNpnkokH1K0ma3SCvWVwJfBT4h2dpJHk9sAZ4VVU9leRlrb4KOBt4BfAzwF8k+fm22seAXwOmgLuSbK6q7QPsW5K0j4GFRVXdlmR8n/K/Bj5YVU+1MbtafQ1wXat/I8kkcHJbNllVDwEkua6NNSwkaYiGfc3i54F/kuSOJJ9P8sutvhTY0TduqtVmqz9LkvVJtibZOj09PYDWJWnhGnZYLAaOAU4B3gNcnySHYsNVtaGqJqpqYmxs7FBsUpLUDPt7FlPAp6qqgDuT/BBYAuwElveNW9Zq7KcuSRqSYR9ZfAZ4PUC7gH0E8DiwGTg7yZFJVgArgTuBu4CVSVYkOYLeRfDNQ+5Zkha8gR1ZJLkWeB2wJMkUcDGwEdjYbqf9AbC2HWVsS3I9vQvXe4Dzq+rptp0LgJuBRcDGqto2qJ4lSTMb5N1Q58yy6B2zjL8EuGSG+k3ATYewNUnSAfIb3JKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6jSwsEiyMcmu9qKjfZf9bpJKsqTNJ8llSSaT3JfkpL6xa5M82D5rB9WvJGl2gzyyuBJYvW8xyXLgdODhvvIZ9F6luhJYD1zexh5D7w17rwZOBi5OcvQAe5YkzWBgYVFVtwG7Z1h0KfBeoPpqa4Crqud24KgkxwFvBLZU1e6qegLYwgwBJEkarIG9VnUmSdYAO6vqK0n6Fy0FdvTNT7XabHUJgIc/8AujbuF57/j/9NVRt6DDwNDCIsmLgPfTOwU1iO2vp3cKi+OPP34Qu5CkBWuYd0P9LLAC+EqSvwGWAfck+WlgJ7C8b+yyVput/ixVtaGqJqpqYmxsbADtS9LCNbSwqKqvVtXLqmq8qsbpnVI6qaoeBTYD57a7ok4BnqyqR4CbgdOTHN0ubJ/eapKkIRrkrbPXAl8CTkgylWTdfobfBDwETAJ/BPwbgKraDfwecFf7fKDVJElDNLBrFlV1Tsfy8b7pAs6fZdxGYOMhbU6SdED8BrckqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToN8U97GJLuS3N9X+1CSryW5L8mnkxzVt+x9SSaTfD3JG/vqq1ttMslFg+pXkjS7QR5ZXAms3qe2BXhlVf0i8NfA+wCSrALOBl7R1vnDJIuSLAI+BpwBrALOaWMlSUM0sLCoqtuA3fvU/ryq9rTZ24FlbXoNcF1VPVVV36D3Lu6T22eyqh6qqh8A17WxkqQhGuU1i3cB/6dNLwV29C2barXZ6s+SZH2SrUm2Tk9PD6BdSVq4RhIWSf4DsAe45lBts6o2VNVEVU2MjY0dqs1KkoDFw95hkt8Cfh04raqqlXcCy/uGLWs19lOXJA3JUI8skqwG3gu8qaq+17doM3B2kiOTrABWAncCdwErk6xIcgS9i+Cbh9mzJGmARxZJrgVeByxJMgVcTO/upyOBLUkAbq+qf1VV25JcD2ynd3rq/Kp6um3nAuBmYBGwsaq2DapnSdLMBhYWVXXODOUr9jP+EuCSGeo3ATcdwtYkSQfIb3BLkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKnTwMIiycYku5Lc31c7JsmWJA+2n0e3epJclmQyyX1JTupbZ20b/2CStYPqV5I0u0EeWVwJrN6ndhFwS1WtBG5p8wBn0HuV6kpgPXA59MKF3hv2Xg2cDFy8N2AkScMzsLCoqtuA3fuU1wCb2vQm4Ky++lXVcztwVJLjgDcCW6pqd1U9AWzh2QEkSRqwYV+zOLaqHmnTjwLHtumlwI6+cVOtNlv9WZKsT7I1ydbp6elD27UkLXBzCoskt8yldiCqqoB6LtvYZ3sbqmqiqibGxsYO1WYlSXSERZIfa9cNliQ5ul2gPibJOLP8C7/DY+30Eu3nrlbfCSzvG7es1WarS5KGqOvI4l8CdwMvbz/3fm4EPnoQ+9sM7L2jaW3bzt76ue2uqFOAJ9vpqpuB01tQHQ2c3mqSpCFavL+FVfVh4MNJLqyqjxzIhpNcC7yO3lHJFL27mj4IXJ9kHfBN4G1t+E3AmcAk8D3gvLb/3Ul+D7irjftAVe170VySNGD7DYu9quojSX4FGO9fp6qu2s8658yy6LQZxhZw/izb2QhsnEufkqTBmFNYJLka+FngXuDpVi5g1rCQJD1/zCksgAlgVTsCkCQtMHP9nsX9wE8PshFJ0uFrrkcWS4DtSe4EntpbrKo3DaQrSdJhZa5h8Z8H2YQk6fA217uhPj/oRiRJh6+53g31HX70aI4jgBcCf19VPzWoxiRJh4+5Hln85N7pJKH3lNhTBtWUJOnwcsBPnW2PEf8MvceHS5IWgLmehnpz3+wL6H3v4vsD6UiSdNiZ691Q/7xveg/wN/RORUmSFoC5XrM4b9CNSJIOX3N9+dGyJJ9Osqt9Pplk2aCbkyQdHuZ6gfsT9N458TPt879aTZK0AMw1LMaq6hNVtad9rgR8d6kkLRBzDYtvJXlHkkXt8w7gWwe70yT/Lsm2JPcnuba9vnVFkjuSTCb5kyRHtLFHtvnJtnz8YPcrSTo4cw2Ld9F7q92jwCPAW4DfOpgdJlkK/FtgoqpeCSwCzgZ+H7i0qn4OeAJY11ZZBzzR6pe2cZKkIZprWHwAWFtVY1X1Mnrh8V+ew34XAz+eZDHwInoB9AbghrZ8E3BWm17T5mnLT2vfIpckDclcw+IXq+qJvTPtPdgnHswOq2on8N+Bh+mFxJPA3cC3q2pPGzYFLG3TS4Edbd09bfxL991ukvVJtibZOj09fTCtSZJmMdeweEGSo/fOJDmGuX+h7xnadtYAK+jdWfUTwOqD2Va/qtpQVRNVNTE25rV3STqU5voX/v8AvpTkT9v8W4FLDnKf/xT4RlVNAyT5FHAqcFSSxe3oYRmws43fCSwHptppq5fwHC6uS5IO3JyOLKrqKuDNwGPt8+aquvog9/kwcEqSF7VrD6cB24HP0rtwDrAWuLFNb27ztOW3+i5wSRquOZ9Kqqrt9P5Sf06q6o4kNwD30HvO1JeBDcD/Bq5L8l9b7Yq2yhXA1Ukmgd307pySJA3RQV13eK6q6mLg4n3KDwEnzzD2+/ROe0mSRuSA32chSVp4DAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUaSRhkeSoJDck+VqSB5K8JskxSbYkebD9PLqNTZLLkkwmuS/JSaPoWZIWslEdWXwY+L9V9XLgVcADwEXALVW1ErilzQOcAaxsn/XA5cNvV5IWtqGHRZKXAK+lvTa1qn5QVd8G1gCb2rBNwFlteg1wVfXcDhyV5LihNi1JC9wojixWANPAJ5J8OcnHk/wEcGxVPdLGPAoc26aXAjv61p9qtWdIsj7J1iRbp6enB9i+JC08owiLxcBJwOVVdSLw9/zolBMAVVVAHchGq2pDVU1U1cTY2Ngha1aSNJqwmAKmquqONn8DvfB4bO/ppfZzV1u+E1jet/6yVpMkDcnQw6KqHgV2JDmhlU4DtgObgbWttha4sU1vBs5td0WdAjzZd7pKkjQEi0e03wuBa5IcATwEnEcvuK5Psg74JvC2NvYm4ExgEvheGytJGqKRhEVV3QtMzLDotBnGFnD+oHuSJM3Ob3BLkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKnTyMIiyaIkX07yZ21+RZI7kkwm+ZP2YiSSHNnmJ9vy8VH1LEkL1SiPLN4NPNA3//vApVX1c8ATwLpWXwc80eqXtnGSpCEaSVgkWQb8M+DjbT7AG4Ab2pBNwFltek2bpy0/rY2XJA3JqI4s/ifwXuCHbf6lwLerak+bnwKWtumlwA6AtvzJNv4ZkqxPsjXJ1unp6QG2LkkLz9DDIsmvA7uq6u5Dud2q2lBVE1U1MTY2dig3LUkL3uIR7PNU4E1JzgR+DPgp4MPAUUkWt6OHZcDONn4nsByYSrIYeAnwreG3LUkL19CPLKrqfVW1rKrGgbOBW6vq7cBngbe0YWuBG9v05jZPW35rVdUQW5akBe9w+p7Fvwd+J8kkvWsSV7T6FcBLW/13gItG1J8kLVijOA31D6rqc8Dn2vRDwMkzjPk+8NahNiZJeobD6chCknSYMiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSp1G8g3t5ks8m2Z5kW5J3t/oxSbYkebD9PLrVk+SyJJNJ7kty0rB7lqSFbhRHFnuA362qVcApwPlJVtF7A94tVbUSuIUfvRHvDGBl+6wHLh9+y5K0sI3iHdyPVNU9bfo7wAPAUmANsKkN2wSc1abXAFdVz+3AUUmOG27XkrSwjfSaRZJx4ETgDuDYqnqkLXoUOLZNLwV29K021WqSpCEZWVgkeTHwSeC3q+rv+pdVVQF1gNtbn2Rrkq3T09OHsFNJ0kjCIskL6QXFNVX1qVZ+bO/ppfZzV6vvBJb3rb6s1Z6hqjZU1URVTYyNjQ2ueUlagEZxN1SAK4AHquoP+hZtBta26bXAjX31c9tdUacAT/adrpIkDcHiEezzVOCdwFeT3Ntq7wc+CFyfZB3wTeBtbdlNwJnAJPA94LyhditJGn5YVNVfAZll8WkzjC/g/IE2JUnaL7/BLUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKnTvAmLJKuTfD3JZJKLRt2PJC0k8yIskiwCPgacAawCzkmyarRdSdLCMS/CAjgZmKyqh6rqB8B1wJoR9yRJC0Z6r7g+vCV5C7C6qv5Fm38n8OqquqBvzHpgfZs9Afj60BsdniXA46NuQgfN39/89Xz/3f2jqhqbacHiYXcyKFW1Adgw6j6GIcnWqpoYdR86OP7+5q+F/LubL6ehdgLL++aXtZokaQjmS1jcBaxMsiLJEcDZwOYR9yRJC8a8OA1VVXuSXADcDCwCNlbVthG3NUoL4nTb85i/v/lrwf7u5sUFbknSaM2X01CSpBEyLCRJnQyLecbHnsxfSTYm2ZXk/lH3ogOTZHmSzybZnmRbknePuqdh85rFPNIee/LXwK8BU/TuEjunqraPtDHNSZLXAt8FrqqqV466H81dkuOA46rqniQ/CdwNnLWQ/t/zyGJ+8bEn81hV3QbsHnUfOnBV9UhV3dOmvwM8ACwdbVfDZVjML0uBHX3zUyyw/2ClUUsyDpwI3DHiVobKsJCkOUryYuCTwG9X1d+Nup9hMizmFx97Io1IkhfSC4prqupTo+5n2AyL+cXHnkgjkCTAFcADVfUHo+5nFAyLeaSq9gB7H3vyAHD9An/sybyS5FrgS8AJSaaSrBt1T5qzU4F3Am9Icm/7nDnqpobJW2clSZ08spAkdTIsJEmdDAtJUifDQpLUybCQJHUyLKQhSXJlkre06Y8nWdWm37/PuC+Ooj9pf7x1VhqSJFcCf1ZVN+xT/25VvXg0XUlz45GFNIsk5ya5L8lXklydZDzJra12S5Lj27grk1yW5ItJHuo7ekiSj7b3j/wF8LK+bX8uyUSSDwI/3r7kdU1b9t2+9T+U5P4kX03ym63+urb+DUm+luSa9g1jaWAWj7oB6XCU5BXAfwR+paoeT3IMsAnYVFWbkrwLuAw4q61yHPCrwMvpPYLlBuA3gBOAVcCxwHZgY/9+quqiJBdU1S/N0MabgV8CXgUsAe5KcltbdiLwCuBvgS/Q+4bxXz3nP7g0C48spJm9AfjTqnocoKp2A68B/rgtv5peOOz1mar6YXsZzrGt9lrg2qp6uqr+Frj1AHv41b71HwM+D/xyW3ZnVU1V1Q+Be4HxA9y2dEAMC+nQeKpvehinhPr39zSeJdCAGRbSzG4F3prkpQDtNNQX6T3pF+DtwF92bOM24DeTLGqv5Xz9LOP+X3v89b7+sm/9MXpHKnce4J9DOiT814g0g6raluQS4PNJnga+DFwIfCLJe4Bp4LyOzXya3ums7cDD9J44O5MNwH1J7qmqt++z/muArwAFvLeqHk3y8oP9c0kHy1tnJUmdPA0lSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkTv8fFkmUky6lie0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='condition', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEAN               0\n",
       "MAX                0\n",
       "MIN                0\n",
       "RANGE              0\n",
       "KURT               0\n",
       "                  ..\n",
       "MEAN_ONSET_LOG     0\n",
       "MEAN_ONSET_SQRT    0\n",
       "NasaTLX            0\n",
       "subject_id         0\n",
       "condition          0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['condition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4772, 59)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.drop([\"condition\"]  ,axis=1)\n",
    "x=x.drop([\"subject_id\"]  ,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4772,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.loc[:,'condition'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3817, 1, 59)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             31744     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 44,259\n",
      "Trainable params: 44,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,59),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.9943 - accuracy: 0.5064 - val_loss: 0.8526 - val_accuracy: 0.6220\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.7488 - accuracy: 0.7160 - val_loss: 0.6367 - val_accuracy: 0.7539\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.7802 - val_loss: 0.4888 - val_accuracy: 0.8272\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.8192 - val_loss: 0.3734 - val_accuracy: 0.8743\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8625 - val_loss: 0.2881 - val_accuracy: 0.9173\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8942 - val_loss: 0.2218 - val_accuracy: 0.9403\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.2647 - accuracy: 0.9151 - val_loss: 0.1733 - val_accuracy: 0.9665\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2137 - accuracy: 0.9379 - val_loss: 0.1330 - val_accuracy: 0.9801\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9489 - val_loss: 0.1054 - val_accuracy: 0.9874\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9557 - val_loss: 0.0805 - val_accuracy: 0.9906\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1286 - accuracy: 0.9628 - val_loss: 0.0671 - val_accuracy: 0.9937\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9678 - val_loss: 0.0516 - val_accuracy: 0.9948\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.9735 - val_loss: 0.0422 - val_accuracy: 0.9937\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9801 - val_loss: 0.0335 - val_accuracy: 0.9969\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9824 - val_loss: 0.0293 - val_accuracy: 0.9969\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9838 - val_loss: 0.0216 - val_accuracy: 0.9990\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9861 - val_loss: 0.0175 - val_accuracy: 0.9990\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9848 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9903 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9924 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9921 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0373 - accuracy: 0.9911 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.9898 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9932 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9953 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9955 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9948 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9932 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9942 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.9955 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.9955 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9961 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.9955 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.9950 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9971 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0158 - accuracy: 0.9963 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0127 - accuracy: 0.9979 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.9955 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9974 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 7.7449e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 9.1522e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9976 - val_loss: 8.3617e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9984 - val_loss: 8.9925e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9974 - val_loss: 7.9834e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 6.9267e-04 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 6.3918e-04 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 0.9976 - val_loss: 5.8099e-04 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 4.7119e-04 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 4.7233e-04 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 4.5497e-04 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 4.4929e-04 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9992 - val_loss: 3.8200e-04 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9979 - val_loss: 4.2675e-04 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 5.4090e-04 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 5.1952e-04 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 4.8206e-04 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 5.0660e-04 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9974 - val_loss: 4.3809e-04 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 3.4340e-04 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 4.5562e-04 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 5.1739e-04 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 3.8977e-04 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9992 - val_loss: 4.6088e-04 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 2.1647e-04 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 1.9575e-04 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 2.0972e-04 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 1.7454e-04 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 1.6594e-04 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 2.1230e-04 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 1.6371e-04 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 4.7014e-04 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 1.7469e-04 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 2.0776e-04 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 1.9862e-04 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 1.2718e-04 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 1.5563e-04 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 1.4113e-04 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 1.0855e-04 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0052 - val_accuracy: 0.9990\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 1.2423e-04 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 1.0547e-04 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 7.0414e-04 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 3.2942e-04 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 1.6049e-04 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 1.4793e-04 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 1.4106e-04 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 1.3261e-04 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 8.6232e-05 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 1.0138e-04 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 9.4707e-05 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 9.6698e-05 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 7.0824e-05 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9990 - val_loss: 7.6212e-05 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 8.4761e-05 - val_accuracy: 1.0000\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 8.4761e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(955,)\n",
      "(955,)\n",
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(expected_classes.shape)\n",
    "print(predict_classes.shape)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
